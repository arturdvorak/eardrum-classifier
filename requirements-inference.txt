# ONNX Runtime Inference Service Dependencies
# Lightweight dependencies for production inference API
# Install with: pip install -r requirements-inference.txt

# Core inference libraries
onnxruntime>=1.16.0
onnx>=1.14.0

# Web framework
fastapi>=0.104.0
uvicorn[standard]>=0.24.0
python-multipart>=0.0.6

# Image processing
Pillow>=9.0.0
opencv-python>=4.8.0

# Data handling
numpy>=1.21.0
pandas>=1.3.0

# HTTP client for URL predictions
requests>=2.28.0

# Optional GPU support (uncomment if using GPU)
# onnxruntime-gpu>=1.16.0

# Development and testing
pytest>=7.0.0
pytest-asyncio>=0.21.0
httpx>=0.24.0
